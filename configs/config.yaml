# Model Configuration
model:
  # VLM Planner
  vlm:
    model_name: "/workspace/brain_robot/models/qwen2.5-vl-7b"
    device: "cuda:0"
    max_new_tokens: 512
    temperature: 0.1
    replan_every: 10  # Steps between LLM calls

  # Brain-Inspired Action Generator
  action_generator:
    plan_dim: 128
    proprio_dim: 15
    action_dim: 7
    chunk_size: 10
    hidden_dim: 128
    num_primitives: 8

    # Primitive types (for initialization)
    primitive_names:
      - "move_left"
      - "move_right"
      - "move_forward"
      - "move_backward"
      - "move_up"
      - "move_down"
      - "open_gripper"
      - "close_gripper"

  # Forward Model (Cerebellum)
  forward_model:
    hidden_dim: 128
    num_layers: 2
    pretrain_epochs: 10

# Environment
env:
  task_suite: "libero_spatial"
  task_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # All tasks
  max_episode_steps: 300
  action_scale: 1.0

# Training
training:
  # RL (PPO)
  num_episodes: 5000
  episodes_per_update: 10
  ppo_epochs: 4
  batch_size: 64
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 1.0

  # Learning rates
  lr_actor: 3e-4
  lr_critic: 3e-4
  lr_forward_model: 1e-3

  # Checkpointing
  save_every: 100
  eval_every: 50
  checkpoint_dir: "./checkpoints"

# Reward Shaping
reward:
  task_success: 100.0
  direction_following: 2.0
  speed_matching: 0.5
  gripper_consistency: 1.0
  forward_model_bonus: 0.5  # Reward for predictable movements
  time_penalty: 0.01
  collision_penalty: 1.0

# Logging
logging:
  use_wandb: true
  project: "brain-robot"
  entity: null  # Your W&B username
